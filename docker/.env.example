# Environment variables for Fantastic Router

# =============================================================================
# LLM API Keys (choose one or more)
# =============================================================================

# OpenAI (GPT models)
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-3.5-turbo-1106
OPENAI_MAX_TOKENS=1000

# Google Gemini (fastest and often cheapest cloud option)
GEMINI_API_KEY=your-google-ai-api-key-here
GOOGLE_AI_API_KEY=your-google-ai-api-key-here
GEMINI_MODEL=gemini-1.5-flash

# Anthropic Claude (if you have direct API access)
ANTHROPIC_API_KEY=your-anthropic-api-key-here

# =============================================================================
# Local LLM Configuration (Ollama)
# =============================================================================

# Ollama server URL (default: http://localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434
# Recommended: mistral-small3.2:24b, llama3.1:8b, llama3.1:1b
OLLAMA_MODEL=llama3.1:8b

# =============================================================================
# Database Configuration
# =============================================================================

DATABASE_URL=postgresql://fantastic_user:fantastic_pass@postgres:5432/property_mgmt
DB_MAX_CONNECTIONS=10
DB_TIMEOUT=30

# =============================================================================
# Application Configuration
# =============================================================================

APP_ENV=development
LOG_LEVEL=INFO
USE_FAST_PLANNER=true
LLM_TIMEOUT=60
LLM_TEMPERATURE=0.1

# =============================================================================
# Setup Instructions
# =============================================================================
# 1. Choose your LLM provider:
#    - Cloud: Set OPENAI_API_KEY or GEMINI_API_KEY
#    - Local: Install Ollama and set OLLAMA_MODEL
# 2. Test: make test-real or make test-all-llms